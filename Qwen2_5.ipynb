{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rMp4rTWk4TKZ"
      },
      "source": [
        "## 安装 LLaMA Factory 依赖"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yQDp0sXX3qE4",
        "outputId": "eac6682b-7c40-4b82-d5b6-55d927534baa"
      },
      "outputs": [],
      "source": [
        "%cd /content/\n",
        "%rm -rf LLaMA-Factory\n",
        "!git clone --depth 1 https://github.com/hiyouga/LLaMA-Factory.git\n",
        "%cd LLaMA-Factory\n",
        "%ls\n",
        "!pip install pydantic==2.10.6\n",
        "!pip install -e .[torch,bitsandbytes]\n",
        "# !pip install --no-deps -e ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cA7vWZ6om3cR"
      },
      "source": [
        "## 使用 LLaMA Board Web UI 微调模型"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "YIfzFgLsm2kS",
        "outputId": "6ef3276a-d551-46ab-8b0f-ebaf44e5fe2b"
      },
      "outputs": [],
      "source": [
        "%cd /content/LLaMA-Factory/\n",
        "!GRADIO_SHARE=1 llamafactory-cli webui"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B6ap81295trx"
      },
      "source": [
        "## 使用命令行微调模型\n",
        "\n",
        "微调过程大约需要 30 分钟。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "psywJyo75vt6",
        "outputId": "3f262072-3254-4332-b2d0-2d7c4f212467"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "args = dict(\n",
        "  stage=\"sft\",                                               # 进行指令监督微调\n",
        "  do_train=True,\n",
        "  model_name_or_path=\"Qwen/Qwen2.5-7B-Instruct\",\n",
        "  dataset=\"yue_1,yue_2\",\n",
        "  template=\"alpaca\",                                         # 使用 llama3 提示词模板\n",
        "  finetuning_type=\"lora\",                                    # 使用 LoRA 适配器来节省显存\n",
        "  lora_target=\"all\",                                         # 添加 LoRA 适配器至全部线性层\n",
        "  output_dir=\"Qwen2.5_lora\",                                  # 保存 LoRA 适配器的路径\n",
        "  per_device_train_batch_size=2,                             # 批处理大小\n",
        "  gradient_accumulation_steps=4,                             # 梯度累积步数\n",
        "  lr_scheduler_type=\"cosine\",                                # 使用余弦学习率退火算法\n",
        "  logging_steps=5,                                           # 每 5 步输出一个记录\n",
        "  warmup_ratio=0.1,                                          # 使用预热学习率\n",
        "  save_steps=1000,                                           # 每 1000 步保存一个检查点\n",
        "  learning_rate=5e-5,                                        # 学习率大小\n",
        "  num_train_epochs=1.0,                                      # 训练轮数\n",
        "  max_samples=1000,                                           # 使用每个数据集中的 10000 条样本\n",
        "  max_grad_norm=1.0,                                         # 将梯度范数裁剪至 1.0\n",
        "  loraplus_lr_ratio=16.0,                                    # 使用 LoRA+ 算法并设置 lambda=16.0\n",
        "  fp16=True,                                                 # 使用 float16 混合精度训练\n",
        "  report_to=\"none\",                                          # 关闭 wandb 记录器\n",
        ")\n",
        "\n",
        "json.dump(args, open(\"train.json\", \"w\", encoding=\"utf-8\"), indent=2)\n",
        "\n",
        "%cd /content/LLaMA-Factory/\n",
        "\n",
        "!llamafactory-cli train train.json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "otpDQuzaMBpm"
      },
      "source": [
        "## 模型推理"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kbFsAE-y5so4"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append('/content/LLaMA-Factory/src')\n",
        "from llamafactory.chat import ChatModel\n",
        "from llamafactory.extras.misc import torch_gc\n",
        "\n",
        "%cd /content/LLaMA-Factory/\n",
        "\n",
        "args = dict(\n",
        "  model_name_or_path=\"Qwen/Qwen2.5-7B-Instruct\", # 使用 4 比特量化版 Llama-3-8b-Instruct 模型\n",
        "  adapter_name_or_path=\"llama3_lora\",                        # 加载之前保存的 LoRA 适配器\n",
        "  template=\"llama3\",                                         # 和训练保持一致\n",
        "  finetuning_type=\"lora\",                                    # 和训练保持一致\n",
        ")\n",
        "chat_model = ChatModel(args)\n",
        "\n",
        "messages = []\n",
        "print(\"使用 `clear` 清除对话历史，使用 `exit` 退出程序。\")\n",
        "while True:\n",
        "  query = input(\"\\nUser: \")\n",
        "  if query.strip() == \"exit\":\n",
        "    break\n",
        "  if query.strip() == \"clear\":\n",
        "    messages = []\n",
        "    torch_gc()\n",
        "    print(\"对话历史已清除\")\n",
        "    continue\n",
        "\n",
        "  messages.append({\"role\": \"user\", \"content\": query})\n",
        "  print(\"Assistant: \", end=\"\", flush=True)\n",
        "\n",
        "  response = \"\"\n",
        "  for new_text in chat_model.stream_chat(messages):\n",
        "    print(new_text, end=\"\", flush=True)\n",
        "    response += new_text\n",
        "  print()\n",
        "  messages.append({\"role\": \"assistant\", \"content\": response})\n",
        "\n",
        "torch_gc()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_7g7kprbhXo0"
      },
      "source": [
        "## 合并 LoRA 权重并上传模型\n",
        "\n",
        "注意：Colab 免费版仅提供了 12GB 系统内存，而合并 8B 模型的 LoRA 权重需要至少 18GB 系统内存，因此你 **无法** 在免费版运行此功能。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XA2kyAz-hXbp",
        "outputId": "3b8d41f4-6291-45f6-d5ee-e16b6efad4c7"
      },
      "outputs": [],
      "source": [
        "!huggingface-cli login"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "eERYoAOrhpcu",
        "outputId": "d8c21ed4-3bbf-4c56-d30b-9dc7663b2141"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "args = dict(\n",
        "  model_name_or_path=\"Qwen/Qwen2.5-7B-Instruct\", # 使用非量化的官方 Llama-3-8B-Instruct 模型\n",
        "  adapter_name_or_path=\"Qwen2.5_lora\",                       # 加载之前保存的 LoRA 适配器\n",
        "  template=\"alpaca\",                                        # 和训练保持一致\n",
        "  finetuning_type=\"lora\",                                   # 和训练保持一致\n",
        "  export_dir=\"Qwen_lora_merged\",                          # 合并后模型的保存目录\n",
        "  export_size=2,                                            # 合并后模型每个权重文件的大小（单位：GB）\n",
        "  export_device=\"cpu\",                                      # 合并模型使用的设备：`cpu` 或 `auto`\n",
        "  export_hub_model_id=\"Nin8520/Qwen2.5-7B\",               # 用于上传模型的 HuggingFace 模型 ID\n",
        ")\n",
        "\n",
        "json.dump(args, open(\"merge.json\", \"w\", encoding=\"utf-8\"), indent=2)\n",
        "\n",
        "%cd /content/LLaMA-Factory/\n",
        "\n",
        "!llamafactory-cli export merge.json"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
